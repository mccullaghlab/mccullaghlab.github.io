{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00175bea",
   "metadata": {},
   "source": [
    "# Week 5 Hands-On: HPC Fundamentals — Slurm, Modules, and Storage\n",
    "\n",
    "---\n",
    "## Overview\n",
    "\n",
    "Today you will practice the core HPC skills you will use for the rest of the semester:\n",
    "\n",
    "1. **Understand the HPC file systems** (home vs project vs scratch) and what belongs where.\n",
    "2. **Inspect hardware and environment** (CPU/RAM, node types, partitions).\n",
    "3. **Use the module system** to load compilers / MPI / chemistry codes / Python stacks.\n",
    "4. **Run jobs with Slurm**:\n",
    "   - interactive jobs (`salloc`, `srun`)\n",
    "   - batch jobs (`sbatch`)\n",
    "   - monitoring and accounting (`squeue`, `sacct`)\n",
    "   - canceling / troubleshooting (`scancel`, common failure modes)\n",
    "5. **Best practices** for reproducibility and performance (job scripts, environment capture, I/O hygiene).\n",
    "\n",
    "At the end there is a **short graded activity** you will turn in.\n",
    "\n",
    "> **Important:** Most code cells below contain commands you should run in a **terminal on pete** (or a Jupyter terminal on pete).  \n",
    "> If you copy/paste, read the comments first and adapt paths/partition names to your account.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f956dc86",
   "metadata": {},
   "source": [
    "## 1. HPC Architecture\n",
    "\n",
    "Most High Performance Computing (HPC) systems, or clusters, are setup with a specific architecture.  A headnode, or login node, is the computer that users log into.  The headnode is used almost solely for logging in, creating file structures, and minor tweaking of input files.  The major computation is done on compute nodes that are connected to the headnode.  Compute nodes are solely for running computation.  They are basically standalone computers with the sole purpose of running extensive simulations.  \n",
    "\n",
    "The headnode controls compute nodes through a piece of software called the queueing system or scheduler.  On Pete, the queueing system is called **slurm**.   Each job on an HPC system is submitted to the queueing system and then resources (compute nodes) are assigned to the submitted jobs as need be.  So, a job request must specicify information like how long it will take, what type of hardware is needed, and how much memory is needed.  \n",
    "\n",
    "Compute nodes are not alsways identical.  They can differ by the hardware they have available on them.  Often, similar compute nodes are grouped into what is called a partition.  Pete has partitions for GPUs, large memory, and different CPUs.  \n",
    "\n",
    "![image.png](img/primary-elements-of-hpc-architecture.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a286134",
   "metadata": {},
   "source": [
    "## 2. Storage & Filesystems: Home vs Scratch (and Why You Should Care)\n",
    "\n",
    "On most clusters you will see at least two important areas:\n",
    "\n",
    "- **Home (`$HOME`)**  \n",
    "  Backed up (sometimes), quota-limited, good for: source code, small inputs, scripts, notebooks, results you want to keep.\n",
    "- **Scratch (`$SCRATCH` or `/scratch/$USER/...`)**  \n",
    "  *Fast* and designed for heavy I/O, good for: trajectories, temporary simulation files, large outputs.  \n",
    "  Often **not backed up** and may be **purged** after some number of days.\n",
    "\n",
    "### Quick checks\n",
    "\n",
    "Run the commands below and write down:\n",
    "- your `$HOME` path\n",
    "- your scratch path (if defined)\n",
    "- your current quotas (if available)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22caf0b9-2cbb-419d-861e-eadb01167148",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "echo \"HOME=$HOME\"\n",
    "echo \"USER=$USER\"\n",
    "echo \"SCRATCH=$SCRATCH\"   # may be blank on some systems\n",
    "\n",
    "# What filesystems exist, and how full are they?\n",
    "df -h | head -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2746bf3",
   "metadata": {},
   "source": [
    "### Where *not* to run big jobs\n",
    "\n",
    "Do **not** run long computations on the login node.  \n",
    "Use Slurm (interactive or batch) so that your job lands on compute nodes.\n",
    "\n",
    "Also: avoid writing huge files into `$HOME`. Put large I/O in scratch.\n",
    "\n",
    "### Create a scratch work folder\n",
    "\n",
    "If `$SCRATCH` is set, do this:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c2a8537-0364-40d5-9214-4e6506bd2323",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "# If SCRATCH is empty, ask the instructor what scratch path to use (often /scratch/$USER)\n",
    "if [ -n \"$SCRATCH\" ]; then\n",
    "  mkdir -p $SCRATCH/week5\n",
    "  echo \"Made: $SCRATCH/week5\"\n",
    "  ls -la $SCRATCH/week5\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617b152",
   "metadata": {},
   "source": [
    "### Best practice pattern (very common)\n",
    "\n",
    "1. Keep your **job script** in your home directory tree\n",
    "2. In the job script, create a unique run directory in scratch, e.g.\n",
    "   - `$SCRATCH/week5/run_$SLURM_JOB_ID`\n",
    "3. Copy needed input files from home → scratch\n",
    "4. Run in scratch\n",
    "5. Copy final results back scratch → home\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08697956",
   "metadata": {},
   "source": [
    "## 3. Hardware Architecture & Node Types\n",
    "\n",
    "Before requesting resources, it helps to know what the cluster offers.\n",
    "\n",
    "### Inspect the login node (just for information)\n",
    "\n",
    "This tells you about the machine you are currently on (often a login node):\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c2e874-7dde-4ceb-aab2-e72c536c496f",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "hostname\n",
    "uname -a\n",
    "\n",
    "# CPU info\n",
    "lscpu | head -n 40\n",
    "\n",
    "# Memory info\n",
    "free -h\n",
    "\n",
    "# If available:\n",
    "# lsmem | head -n 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06c17e",
   "metadata": {},
   "source": [
    "### What compute nodes exist? (Slurm partitions)\n",
    "\n",
    "Slurm groups nodes into **partitions** (sometimes called \"queues\").\n",
    "Use `sinfo` to see partitions, node states, and time limits:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73148fda-2f5d-4625-95d6-25fffa544195",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "sinfo\n",
    "# More readable:\n",
    "sinfo -o \"%P %a %l %D %t %N\" | head -n 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec408d",
   "metadata": {},
   "source": [
    "Write down at least **two partition names** you see (e.g., `short`, `long`, `gpu`, etc.).  \n",
    "You will use them in the graded activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf5e91",
   "metadata": {},
   "source": [
    "## 4. Environment Modules (and Why `module load` Matters)\n",
    "\n",
    "Clusters typically manage software using **Environment Modules** (or Lmod).  \n",
    "A module adjusts your `PATH`, `LD_LIBRARY_PATH`, etc., so you can access a consistent toolchain.\n",
    "\n",
    "### Common module commands\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56ec4baa-d755-4a34-ab3a-794176a89f31",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "module avail | head -n 40\n",
    "module list\n",
    "\n",
    "# Search for things (examples)\n",
    "module spider python 2>/dev/null | head -n 40\n",
    "module spider amber 2>/dev/null | head -n 40\n",
    "module spider gaussian 2>/dev/null | head -n 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3b6ac",
   "metadata": {},
   "source": [
    "### Loading a module\n",
    "\n",
    "Try loading a Python module (names vary by cluster):\n",
    "\n",
    "> If the exact module name differs, use `module avail` / `module spider` to find the right one.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eb76648-7a79-4b12-b7de-7c4b6c58fff8",
   "metadata": {},
   "source": [
    "# Example (edit as needed)\n",
    "# module load python/3.11\n",
    "# module load anaconda3\n",
    "\n",
    "module list\n",
    "python --version || which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29660b0",
   "metadata": {},
   "source": [
    "### Reproducibility tip\n",
    "\n",
    "In job scripts, always record your environment at the top:\n",
    "\n",
    "- `module purge`\n",
    "- `module load ...`\n",
    "- `module list`\n",
    "- `python --version`, `which python`, etc.\n",
    "\n",
    "That way you can reproduce runs later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceecf77",
   "metadata": {},
   "source": [
    "## 5. Slurm Basics: The Mental Model\n",
    "\n",
    "- **You do not \"run on the cluster\"**; you request resources from Slurm.\n",
    "- Slurm decides *where/when* your job runs based on availability and policies.\n",
    "\n",
    "Key vocabulary:\n",
    "\n",
    "- **job**: a request to run something\n",
    "- **partition**: a queue / group of nodes\n",
    "- **node**: a compute machine\n",
    "- **task**: a process (often `MPI rank`)\n",
    "- **CPU per task**: threads for that process\n",
    "- **time limit**: maximum runtime you request\n",
    "\n",
    "### Your most-used commands\n",
    "\n",
    "- `squeue -u $USER` : see your jobs\n",
    "- `sbatch job.slurm` : submit a batch job\n",
    "- `sacct -j <jobid> --format=...` : see finished job info\n",
    "- `scancel <jobid>` : cancel a job\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8177857-6125-4b0d-8a2c-2229fc8be9ab",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c7032",
   "metadata": {},
   "source": [
    "## 6. Interactive Jobs (Good for Testing)\n",
    "\n",
    "Interactive jobs are great for:\n",
    "\n",
    "- testing imports / small scripts\n",
    "- debugging before a longer `sbatch`\n",
    "- quick visualization on compute nodes\n",
    "\n",
    "Two common patterns:\n",
    "\n",
    "### A) Allocate then run\n",
    "- `salloc ...`\n",
    "- then `srun ...`\n",
    "\n",
    "### B) One-shot run\n",
    "- `srun ... <command>`\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34c6f56a-3361-4fb1-a808-eb69db5d4304",
   "metadata": {},
   "source": [
    "# Example interactive allocation (EDIT partition/time as needed)\n",
    "# salloc -p <partition_name> -t 00:10:00 -N 1 -n 1 --mem=2G\n",
    "\n",
    "# Once allocated, run something on the compute node:\n",
    "# srun hostname\n",
    "# srun python --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503241c",
   "metadata": {},
   "source": [
    "If you do an interactive allocation, remember to exit when done:\n",
    "\n",
    "- type `exit` in the allocated shell, or\n",
    "- press Ctrl-D\n",
    "\n",
    "This releases resources for others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3fb59",
   "metadata": {},
   "source": [
    "## 7. Batch Jobs with `sbatch`: A Minimal Working Example\n",
    "\n",
    "We will run a simple Python calculation under Slurm and capture:\n",
    "\n",
    "- which node it ran on\n",
    "- how long it took\n",
    "- memory usage (via Slurm accounting)\n",
    "\n",
    "### 7.1 Create a job script\n",
    "\n",
    "In `/projects/tia001/$USER/week5`, create a file named `hello_slurm.slurm`.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46406feb-b7c6-46a1-8a5c-52a440ade2ee",
   "metadata": {},
   "source": [
    "# Create the script using a heredoc (run in terminal on pete)\n",
    "cat > hello_slurm.slurm <<'EOF'\n",
    "#!/bin/bash\n",
    "#SBATCH -J hello_slurm\n",
    "#SBATCH -o hello_slurm_%j.out\n",
    "#SBATCH -e hello_slurm_%j.err\n",
    "#SBATCH -p <PARTITION>          # <-- EDIT THIS\n",
    "#SBATCH -t 00:02:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 1\n",
    "#SBATCH --mem=1G\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"Job ID: $SLURM_JOB_ID\"\n",
    "echo \"Node list: $SLURM_NODELIST\"\n",
    "echo \"Running on: $(hostname)\"\n",
    "echo \"Start time: $(date)\"\n",
    "\n",
    "module purge\n",
    "# module load python/3.11   # <-- EDIT/UNCOMMENT as needed\n",
    "module list || true\n",
    "which python || true\n",
    "python --version || true\n",
    "\n",
    "# A tiny computation (Monte Carlo estimate of pi)\n",
    "python - <<'PY'\n",
    "import random, time\n",
    "t0=time.time()\n",
    "n=2_000_00  # 200k\n",
    "inside=0\n",
    "for _ in range(n):\n",
    "    x=random.random(); y=random.random()\n",
    "    inside += (x*x+y*y) <= 1.0\n",
    "pi=4*inside/n\n",
    "print(f\"pi ~ {pi:.6f} using n={n}\")\n",
    "print(f\"python_time_s = {time.time()-t0:.3f}\")\n",
    "PY\n",
    "\n",
    "echo \"End time: $(date)\"\n",
    "EOF\n",
    "\n",
    "# Show the file\n",
    "sed -n '1,120p' hello_slurm.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa60748",
   "metadata": {},
   "source": [
    "### 7.2 Submit the job\n",
    "\n",
    "Replace `<PARTITION>` in the script with a real partition name from `sinfo`, then submit:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f837c77-8f87-476c-9ed1-b2ebd9108eea",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "# (1) edit the partition line\n",
    "# nano hello_slurm.slurm\n",
    "# or: vi hello_slurm.slurm\n",
    "\n",
    "# (2) submit\n",
    "sbatch hello_slurm.slurm\n",
    "\n",
    "# (3) check queue\n",
    "squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07fdc9b",
   "metadata": {},
   "source": [
    "### 7.3 Inspect output\n",
    "\n",
    "When the job finishes, you should see files like:\n",
    "\n",
    "- `hello_slurm_<jobid>.out`\n",
    "- `hello_slurm_<jobid>.err`\n",
    "\n",
    "View them:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7846d2b0-f0c8-4e52-ab2d-02276dad54fe",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "ls -lt hello_slurm_*.out | head\n",
    "# Replace <jobid> with your job id:\n",
    "# cat hello_slurm_<jobid>.out\n",
    "# cat hello_slurm_<jobid>.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa841f8",
   "metadata": {},
   "source": [
    "## 8. Job Accounting (`sacct`) and Common Failure Modes\n",
    "\n",
    "### 8.1 `sacct` basics\n",
    "\n",
    "After a job finishes, Slurm can report runtime, memory, exit code, etc.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07f33d51-c750-4e45-aab7-493b4c9ce607",
   "metadata": {},
   "source": [
    "# Run in terminal on pete\n",
    "# Replace <jobid>:\n",
    "# sacct -j <jobid> --format=JobID,JobName%20,State,ExitCode,Elapsed,MaxRSS,AllocCPUS,ReqMem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36106ac",
   "metadata": {},
   "source": [
    "### 8.2 Common failure modes to recognize\n",
    "\n",
    "- **`PartitionTimeLimit`**: you requested more time than the partition allows\n",
    "- **`QOSMaxWallDurationPerJobLimit`**: similar, but policy/QoS enforced\n",
    "- **`OUT_OF_MEMORY` / `OOM`**: your job exceeded requested memory\n",
    "- **`CANCELLED`**: you (or admin) cancelled it\n",
    "- **`FAILED`**: non-zero exit code or node failure\n",
    "\n",
    "Best practice: if a job dies, first check:\n",
    "1. the `.err` file\n",
    "2. the last ~20 lines of `.out`\n",
    "3. `sacct` for State/ExitCode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5bcb9",
   "metadata": {},
   "source": [
    "## 9. Job Arrays (Running Many Similar Jobs)\n",
    "\n",
    "Job arrays are ideal when you have many independent calculations (e.g., scanning dihedral angles, running multiple replicas).\n",
    "\n",
    "Example idea: run 10 short tasks with different random seeds.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a7a8620-4392-4b94-b351-5a2d51858041",
   "metadata": {},
   "source": [
    "# Example array script (DO NOT submit unless instructed)\n",
    "cat > array_example.slurm <<'EOF'\n",
    "#!/bin/bash\n",
    "#SBATCH -J array_demo\n",
    "#SBATCH -o array_demo_%A_%a.out\n",
    "#SBATCH -e array_demo_%A_%a.err\n",
    "#SBATCH -p <PARTITION>      # EDIT\n",
    "#SBATCH -t 00:02:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 1\n",
    "#SBATCH --mem=1G\n",
    "#SBATCH --array=1-10\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "seed=$SLURM_ARRAY_TASK_ID\n",
    "python - <<PY\n",
    "import random\n",
    "random.seed($seed)\n",
    "print(\"task_id\", $seed, \"rand\", random.random())\n",
    "PY\n",
    "EOF\n",
    "\n",
    "sed -n '1,120p' array_example.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5a839",
   "metadata": {},
   "source": [
    "If you submit an array, you will see multiple tasks under one job allocation ID.  \n",
    "Use `squeue -u $USER` to monitor them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f8250",
   "metadata": {},
   "source": [
    "# Graded Activity (Turn In)\n",
    "\n",
    "**Goal:** demonstrate you can (1) identify cluster partitions, (2) submit a Slurm batch job that runs on a compute node, and (3) retrieve accounting information.\n",
    "\n",
    "## What to turn in\n",
    "\n",
    "Create a folder named `/projects/tia001/$USER/week5` containing:\n",
    "\n",
    "1. `activity_job.slurm` — your batch script  \n",
    "2. `activity_job.out` — the Slurm output file from your run (rename it)  \n",
    "3. `activity_sacct.txt` — a text file containing the `sacct` line(s) for your job  \n",
    "4. `answers.txt` — short answers to the questions below\n",
    "\n",
    "---\n",
    "\n",
    "## Step A. Partition scavenger hunt (answers go in `answers.txt`)\n",
    "\n",
    "1. List **two partition names** on the cluster.  \n",
    "2. For one of those partitions, what is the **maximum time limit** (walltime) shown by `sinfo`?\n",
    "\n",
    "Helpful commands:\n",
    "\n",
    "```bash\n",
    "sinfo\n",
    "sinfo -o \"%P %a %l %D %t %N\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step B. Run a real Slurm job\n",
    "\n",
    "Write `activity_job.slurm` that:\n",
    "\n",
    "- requests **1 node**, **1 task**, **1 CPU**  \n",
    "- requests **1–2 GB** memory  \n",
    "- requests **≤ 5 minutes** walltime  \n",
    "- runs on a real partition on pete\n",
    "- creates a scratch run directory (if scratch is available)\n",
    "- runs the Monte Carlo pi code (or any small Python compute)\n",
    "- copies the final output back to your home folder\n",
    "\n",
    "You may adapt `hello_slurm.slurm` from above.\n",
    "\n",
    "Submit:\n",
    "\n",
    "```bash\n",
    "sbatch activity_job.slurm\n",
    "```\n",
    "\n",
    "When it finishes, rename your output file to `activity_job.out`.\n",
    "\n",
    "---\n",
    "\n",
    "## Step C. Accounting\n",
    "\n",
    "Use `sacct` to record job accounting information in `activity_sacct.txt`:\n",
    "\n",
    "- Elapsed time\n",
    "- MaxRSS (maximum memory)\n",
    "- State and ExitCode\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "sacct -j <jobid> --format=JobID,JobName%20,State,ExitCode,Elapsed,MaxRSS,AllocCPUS,ReqMem\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal grading checklist (10 pts)\n",
    "\n",
    "- (2) `answers.txt` has correct partition names + one time limit  \n",
    "- (4) `activity_job.slurm` is valid, requests reasonable resources, and runs on compute nodes  \n",
    "- (2) `activity_job.out` shows node name + pi estimate  \n",
    "- (2) `activity_sacct.txt` includes the requested fields for your job  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f372f",
   "metadata": {},
   "source": [
    "## Optional (Nice-to-know) Extras\n",
    "\n",
    "If time allows, explore one or two of these and jot notes for yourself:\n",
    "\n",
    "- `scontrol show job <jobid>` (very detailed job info)\n",
    "- `seff <jobid>` (if installed; summarizes efficiency)\n",
    "- `ulimit -a` (limits)\n",
    "- environment capture:\n",
    "  - `env | sort > env_<jobid>.txt`\n",
    "  - `pip freeze` / `conda env export`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
